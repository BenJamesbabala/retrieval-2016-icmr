<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Bags of Local Convolutional Features for Scalable Instance Search by xavigiro</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Bags of Local Convolutional Features for Scalable Instance Search</h1>
      <h2 class="project-tagline">ACM International Conference on Multimedia Retrieval 2016 (Best poster award)</h2>
      <a href="https://github.com/xavigiro/BoW_CNN_InstanceSearch" class="btn">View on GitHub</a>
      <a href="https://github.com/xavigiro/BoW_CNN_InstanceSearch/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/xavigiro/BoW_CNN_InstanceSearch/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <table>
<thead>
<tr>
<th align="center"><img src="https://raw.githubusercontent.com/evamohe/BoW_CNN_InstanceSearch/master/authors/logo_1.jpg?token=AHPpwJEFJz9ZB5Q5RyZ_O6HuU7_S2yp-ks5X1vMgwA%3D%3D" alt="Logo Icmr"></th>
<th>Paper accepted at <a href="http://www.icmr2016.com/index.html">The Annual ACM International Conference on Multimedia Retrieval (ICMR) </a>
</th>
</tr>
</thead>
<tbody>
</tbody>
</table>

<table>
<thead>
<tr>
<th align="center"><img src="https://raw.githubusercontent.com/imatge-upc/retrieval-2016-lostobject/master/authors/Eva.jpg?token=AKsMd4iuttxHH44mYL3mPpJEtSvXVXF8ks5Xe-AWwA%3D%3D" alt="Eva Mohedano" title="Eva Mohedano"></th>
<th align="center"><img src="https://raw.githubusercontent.com/evamohe/BoW_CNN_InstanceSearch/master/authors/salvador.jpg?token=AHPpwCjeOzq3duY0lkLTXEe8H7giXJEPks5X1vG2wA%3D%3D" alt="Amaia Salvador" title="Amaia Salvador"></th>
<th align="center"><img src="https://raw.githubusercontent.com/imatge-upc/retrieval-2016-lostobject/master/authors/Kevin.jpg?token=AKsMd4VU31T7Bh8CztufWEWNudazbB_Uks5Xe-AxwA%3D%3D" alt="Kevin McGuinness" title="Kevin McGuinness"></th>
<th align="center"><img src="https://raw.githubusercontent.com/evamohe/BoW_CNN_InstanceSearch/master/authors/giro.jpg?token=AHPpwDdVdPYfMIwMBgHbjK9pPMJva1GOks5X1vHIwA%3D%3D" alt="Xavier Giro-i-Nieto" title="Xavier Giro-i-Nieto"></th>
<th align="center"><img src="https://raw.githubusercontent.com/imatge-upc/retrieval-2016-lostobject/master/authors/Noel.jpg?token=AKsMdyemO5eJke9B9rqdRtA7otJscq1wks5Xe-BEwA%3D%3D" alt="Noel O'Connor" title="Noel O'Connor"></th>
<th align="center"><img src="https://raw.githubusercontent.com/evamohe/BoW_CNN_InstanceSearch/master/authors/marques.jpg?token=AHPpwN3NJU0PqS8i-r2Ng7TpRjlQV22vks5X1vHWwA%3D%3D" alt="Ferran Marques"></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="https://www.insight-centre.org/users/eva-mohedano">Eva Mohedano</a></td>
<td align="center"><a href="https://raw.githubusercontent.com/evamohe/BoW_CNN_InstanceSearch/master/authors/salvador.jpg?token=AHPpwCjeOzq3duY0lkLTXEe8H7giXJEPks5X1vG2wA%3D%3D" title="Amaia Salvador">Amaia Salvador</a></td>
<td align="center"><a href="https://www.insight-centre.org/users/kevin-mcguinness">Kevin McGuinness</a></td>
<td align="center"><a href="https://imatge.upc.edu/web/people/xavier-giro">Xavier Giro-i-Nieto</a></td>
<td align="center"><a href="https://www.insight-centre.org/users/noel-oconnor">Noel O'Connor</a></td>
<td align="center"><a href="https://imatge.upc.edu/web/people/ferran-marques"> Ferran Marques</a></td>
</tr>
</tbody>
</table>

<p>A joint collaboration between:</p>

<table>
<thead>
<tr>
<th align="center"><img src="https://raw.githubusercontent.com/imatge-upc/saliency-2016-cvpr/master/logos/insight.jpg" alt="logo-insight" title="Insight Centre for Data Analytics"></th>
<th align="center"><img src="https://raw.githubusercontent.com/imatge-upc/saliency-2016-cvpr/master/logos/dcu.png" alt="logo-dcu" title="Dublin City University"></th>
<th align="center"><img src="https://raw.githubusercontent.com/imatge-upc/saliency-2016-cvpr/master/logos/upc.jpg" alt="logo-upc" title="Universitat Politecnica de Catalunya"></th>
<th align="center"><img src="https://raw.githubusercontent.com/imatge-upc/saliency-2016-cvpr/master/logos/etsetb.png" alt="logo-etsetb" title="ETSETB TelecomBCN"></th>
<th align="center"><img src="https://raw.githubusercontent.com/imatge-upc/saliency-2016-cvpr/master/logos/gpi.png" alt="logo-gpi" title="UPC Image Processing Group"></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="https://www.insight-centre.org/">Insight Centre for Data Analytics</a></td>
<td align="center"><a href="http://www.dcu.ie/">Dublin City University (DCU)</a></td>
<td align="center"><a href="http://www.upc.edu/?set_language=en">Universitat Politecnica de Catalunya (UPC)</a></td>
<td align="center"><a href="https://www.etsetb.upc.edu/en/">UPC ETSETB TelecomBCN</a></td>
<td align="center"><a href="https://imatge.upc.edu/web/">UPC Image Processing Group</a></td>
</tr>
</tbody>
</table>

<h2>
<a id="abstract" class="anchor" href="#abstract" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Abstract</h2>

<p>This work proposes a simple instance retrieval pipeline based on encoding the convolutional features of CNN using the bag of words aggregation scheme (BoW). Assigning each local array of activations in a convolutional layer to a visual word produces an assignment map, a compact representation that relates regions of an image with a visual word. We use the assignment map for fast spatial reranking, obtaining object localizations that are used for query expansion. We demonstrate the suitability of the BoW representation based on local CNN features for instance retrieval, achieving competitive performance on the Oxford and Paris buildings benchmarks. We show that our proposed system for CNN feature aggregation with BoW outperforms state-of-the-art techniques using sum pooling at a subset of the challenging TRECVid INS benchmark.</p>

<h2>
<a id="publication" class="anchor" href="#publication" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Publication</h2>

<p><a href="http://delivery.acm.org/10.1145/2920000/2912061/p327-mohedano.pdf?ip=136.206.26.116&amp;id=2912061&amp;acc=ACTIVE%20SERVICE&amp;key=846C3111CE4A4710%2E821500BF45340188%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;CFID=834414703&amp;CFTOKEN=41969317&amp;__acm__=1473100476_3638b03bb0e3539dc3119507f650f0cc">Paper link</a></p>

<p><img src="https://raw.githubusercontent.com/evamohe/BoW_CNN_InstanceSearch/master/docs/image.jpeg?token=AHPpwMnNlL9yWB3Lt7588wSrlzim1tO4ks5X1vazwA%3D%3D" alt="Image of the paper"></p>

<p>Please cite with the following Bibtex code:</p>

<pre><code>@inproceedings{Mohedano:2016:BLC:2911996.2912061,
 author = {Mohedano, Eva and McGuinness, Kevin and O'Connor, Noel E. and Salvador, Amaia and Marques, Ferran and Giro-i-Nieto, Xavier},
 title = {Bags of Local Convolutional Features for Scalable Instance Search},
 booktitle = {Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval},
 series = {ICMR '16},
 year = {2016},
 isbn = {978-1-4503-4359-6},
 location = {New York, New York, USA},
 pages = {327--331},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/2911996.2912061},
 doi = {10.1145/2911996.2912061},
 acmid = {2912061},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bag of words, convolutional neural networks, instance retrieval},
} 
</code></pre>

<h2>
<a id="code-instructions" class="anchor" href="#code-instructions" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Code Instructions</h2>

<h3>
<a id="description" class="anchor" href="#description" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Description</h3>

<p>It contains scripts to build Bag of Visual Words based on local CNN features to perform instance search in three different datasets:</p>

<ul>
<li><p><a href="http://www.robots.ox.ac.uk/%7Evgg/data/oxbuildings/">Oxford Buildings</a> (and Oxford 105k).</p></li>
<li><p><a href="http://www.robots.ox.ac.uk/%7Evgg/data/parisbuildings/">Paris Buildings</a> (and Paris 106k).</p></li>
<li><p>Trecvid_subset: Subset of 23.614 keyframes/~13.000video shots from <a href="http://www-nlpir.nist.gov/projects/tv2015/">TRECVid-INS</a> dataset. Keyframes extracted uniformly at 1/4fps. Queries and groundtruth correspond to <a href="http://www-nlpir.nist.gov/projects/tv2013/tv2013.html">INS2013</a>.</p></li>
</ul>

<h3>
<a id="prerequisits" class="anchor" href="#prerequisits" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Prerequisits</h3>

<p>Python packages necessary specified in <em>requirements.txt</em> run:</p>

<pre><code> pip install -r requirements.txt

</code></pre>

<p>It also needs:</p>

<ul>
<li><p><strong><a href="http://caffe.berkeleyvision.org/">caffe</a></strong> with python support</p></li>
<li>
<p><strong><a href="https://github.com/dougalsutherland/vlfeat-ctypes">vlfeat</a></strong> library</p>

<ul>
<li>Once installed, modify 'kmeans.py' file, located in the vlfeat python package: (i.e /usr/local/lib/python2.7/dist-packages/vlfeat_ctypes-0.1.4-py2.7.egg/vlfeat/kmeans.py)
for lib/kmeans.py of this repo.</li>
</ul>
</li>
<li><p><strong>invidx</strong> module. Follow instructions in 'lib/py=inverted-index'/</p></li>
</ul>

<p><strong>NOTE</strong>
You can create a virtual enviroment to set the specific dependences of this project in an independent enviroment, without modifying your original python enviroment. Check <a href="http://docs.python-guide.org/en/latest/dev/virtualenvs/">how to create a virtual enviroment</a>. Using <em>--system-site-packages</em> flag when creating the venv will copy all the packages from your python installation. This will prevent you of re-installing packages in the new virtual enviroment that you have already installed. This will allow you to install only the new packages.</p>

<h3>
<a id="how-to-run-it" class="anchor" href="#how-to-run-it" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>How to run it?</h3>

<p><em>bow_pipeline</em> folder contain the main scripts. Parameters must be specified in the <em>settings.json</em> file located in the folder named as the dataset.</p>

<h4>
<a id="step-1-feature-extraction-bow_pipelinea_feature_extractionpy" class="anchor" href="#step-1-feature-extraction-bow_pipelinea_feature_extractionpy" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Step 1: FEATURE EXTRACTION (bow_pipeline/A_feature_extraction.py)</h4>

<p>This script extracts features from a pre-trained CNN (by default the fully convolutional <a href="https://gist.github.com/ksimonyan/211839e770f7b538e2d8#file-readme-md">VGG-16 network</a>. It computers descriptors from the specified layer/s <em>Layer_output</em> parameter in a levelDB dataset in <em>featuresDB</em> paramenter in the <em>settings.json</em>, with the format [<em>featuresDB</em>]/[<em>layer</em>]_db.</p>

<p><strong>NOTE</strong>
Features are stored as the original dictionary created by the <em>Net</em> class from <a href="http://caffe.berkeleyvision.org/">caffe</a>. For reading, you should use the class <em>Local_Feature_ReaderDB</em> located in <em>bow_pipeline/reader.py</em>. This class contains methods to extract local features in the format (<em>n_samples</em>, <em>n_dimensions</em>)/image for BoW encoding. It also performs SumPooling, generating (<em>1</em>, <em>n_dimensions</em>)/image. It also contains methods to interpolate feature maps when reading. For more info in, check <em>bow_pipeline/reader.py</em> script.</p>

<h4>
<a id="step-2-building-visual-vocabulary-bow_pipelineb_processing_clusteringpy" class="anchor" href="#step-2-building-visual-vocabulary-bow_pipelineb_processing_clusteringpy" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Step 2: BUILDING VISUAL VOCABULARY (bow_pipeline/B_processing_clustering.py)</h4>

<p>It performs the clustering of the local features. It is necessary to set the
following parameters:</p>

<pre><code>TRAIN_PCA=True -- If we want PCA/whitenning (default true)
TRAIN_CENTROIDS=True -- if we want to train centroids (default true)
l2norm=True -- if we want to perform l2-norm on the features (default true)
n_centers=25000 -- # of clusters
pca_dim=512 -- dimensions when doing PCA
</code></pre>

<p>It is necessary to specify the settings.json (different for each dataset, which contains paths for reading features/store models.</p>

<h4>
<a id="step-3-assignments-and-inverted-file-generation-bow_pipelinec_bow_representationpy" class="anchor" href="#step-3-assignments-and-inverted-file-generation-bow_pipelinec_bow_representationpy" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Step 3: ASSIGNMENTS AND INVERTED FILE GENERATION (bow_pipeline/C_bow_representation.py)</h4>

<p>Once the visual vocabulary is build, we can compute the assignments based on the local features/image and we can index the dataset of images. It is necessary to set the following parameters (check the script):</p>

<pre><code>settings file for the dataset
dim_input -- network input dimensions in string format
network="vgg16" -- string with network name to use
list_layers -- list with layer/s to use
new_dim -- tuple with the feature map dimension
</code></pre>

<h4>
<a id="step-4-rankings-generation-bow_pipelined_rankings_bowpy" class="anchor" href="#step-4-rankings-generation-bow_pipelined_rankings_bowpy" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Step 4: RANKINGS GENERATION (bow_pipeline/D_rankings_BoW.py)</h4>

<p>It uses the inverted file generated and generates the rankigns for the queries. It computes the assignments for the query on-the-fly. It is necessary to set parameters as in Step 3, with the additional:</p>

<pre><code>    - masking = 3 (Kind of masking applied on query):
        0== No maks;
        1 == Only consider words from foreground;
        2 == Only consider words from background [CHANGED];
        3 == Apply inverse weight to the foreground object
    - augmentation = [0] [TO REVIEW]
        0 == No augmentation;
        1 == 0+Flipped image;
        2 == 0+Zoomed image (same size as input net, but just capturing center crop
             of the image when it has been zoomed to the double size).
        3 == 0+Flipped zoomed crop
    - QUERY_EXPANSION [TO_ADD]
</code></pre>

<p><strong>NOTE</strong>
If using bow_pipeline/D_rankings_Pooling.py; Skip steps 2 and 3. The whole pipeline is based in sumpooled features (without inverted index and codebook generation).</p>

<p><em>.txt</em> files are generated per query under <em>datasetFolder/lists_[bow/pooling]</em>.</p>

<h4>
<a id="step-5-evaluation-bow_pipelineevaluate_oxf_partrecvidpy" class="anchor" href="#step-5-evaluation-bow_pipelineevaluate_oxf_partrecvidpy" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Step 5: EVALUATION (bow_pipeline/evaluate_[Oxf_Par/trecvid].py)</h4>

<p>It computes mean average precission for rankings generated in Step 4.</p>

<ul>
<li><p><em>evaluate_Oxf_Par</em> for Paris and Oxford datasets</p></li>
<li><p>evaluate_trecvid for TRECVid subset.</p></li>
</ul>

<p><strong>NOTE</strong> It generate <em>map.txt</em> in bow_pipeline folder with results.</p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/xavigiro/BoW_CNN_InstanceSearch">Bags of Local Convolutional Features for Scalable Instance Search</a> is maintained by <a href="https://github.com/xavigiro">xavigiro</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
